# RunPod Deployment Log

## Overview
This document tracks our journey to deploy a simple NVIDIA-SMI endpoint on RunPod Serverless, following official documentation step by step.

**Goal**: Create a working RunPod serverless endpoint that returns `nvidia-smi` output to verify GPU access.

---

## Attempt 1: Complex Setup with FastAPI + Load Balancing (FAILED)
**Date**: Sept 2, 2024  
**Approach**: Tried to create a hybrid FastAPI + RunPod serverless handler

### What We Tried:
- Created `main.py` with both FastAPI endpoints and RunPod handler
- Mixed Load Balancing and Queue-based patterns in same file
- Complex GitHub Actions workflow with GraphQL API calls
- Template creation with `saveTemplate` mutations

### Issues Encountered:
1. **GraphQL API Errors**: 
   - `Cannot query field "serverlessEndpoints" on type "Query"`
   - Should use `{ myself { endpoints { id name } } }`

2. **Template Creation Failures**:
   - Missing required fields: `dockerArgs`, `volumeInGb`
   - Template name uniqueness conflicts

3. **Worker Quota Issues**:
   - Hit 5-worker quota limit
   - `Max workers across all endpoints must not exceed your workers quota (5)`

4. **Endpoint Type Confusion**:
   - Created Load Balancing endpoint but code was Queue-based
   - Mixed `/ping`, `/health` (Load Balancing) with `/runsync` (Queue-based)

### What Didn't Work:
- ‚ùå Complex GitHub Actions with direct RunPod API calls
- ‚ùå Mixing FastAPI with RunPod serverless handler
- ‚ùå Load Balancing endpoints with Queue-based code
- ‚ùå Template management via GraphQL mutations
- ‚ùå Multiple endpoints created due to naming conflicts

### Lessons Learned:
1. RunPod has two distinct endpoint types: **Queue-based** vs **Load Balancing**
2. Don't mix FastAPI patterns with RunPod serverless handlers
3. GraphQL API has specific field names and required parameters
4. Template names must be unique across account
5. Worker quotas are shared across all endpoints

---

## Attempt 2: Official Documentation Approach (IN PROGRESS)
**Date**: Sept 2, 2024  
**Approach**: Following RunPod official documentation step by step

### References Used:
1. https://docs.runpod.io/serverless/workers/overview
2. https://docs.runpod.io/serverless/workers/custom-worker  
3. https://docs.runpod.io/serverless/workers/handler-functions
4. https://docs.runpod.io/serverless/workers/concurrent-handler
5. https://docs.runpod.io/serverless/workers/deploy
6. https://docs.runpod.io/serverless/workers/github-integration

### Current Setup:

#### 1. Simple Handler Function ‚úÖ
**File**: `handler.py`
```python
import runpod
import subprocess

def handler(job):
    try:
        result = subprocess.check_output(['nvidia-smi'], text=True)
        return {"gpu_info": result}
    except Exception as e:
        return {"error": str(e)}

runpod.serverless.start({"handler": handler})
```

**Key Points from Documentation**:
- Handler takes `job` parameter
- Access input via `job["input"]` 
- Use `runpod.serverless.start()` to launch
- Handle errors gracefully
- Initialize heavy resources outside handler

#### 2. Simplified Container ‚úÖ
**File**: `Dockerfile`
```dockerfile
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY handler.py .
CMD ["python", "-u", "handler.py"]
```

**File**: `requirements.txt`
```
runpod>=1.6.2
```

#### 3. Build-Only GitHub Workflow ‚úÖ
**File**: `.github/workflows/build-image.yml`
- Only builds and pushes Docker image to Google Artifact Registry
- Provides deployment instructions in GitHub Actions summary
- **No automatic RunPod API calls** (following documentation recommendation)

#### 4. Manual Deployment Process (PENDING)
**Per RunPod Documentation**:
1. **Build image** via GitHub Actions ‚úÖ
2. **Connect GitHub to RunPod** (Console ‚Üí Settings ‚Üí Connections ‚Üí GitHub)
3. **Create endpoint** via RunPod Console (not API)
4. **Select repository and branch** for auto-deployment
5. **Create GitHub release** to trigger updates

### What We're Doing Right:
- ‚úÖ Following official documentation patterns
- ‚úÖ Simple, focused handler function
- ‚úÖ Minimal dependencies
- ‚úÖ Using RunPod's preferred GitHub integration
- ‚úÖ Avoiding complex GraphQL API management

### Progress Update:
1. ‚úÖ **Push code to trigger image build** - COMPLETED
   - GitHub Actions ran successfully
   - ‚ùå GitHub Actions output had incorrect SHA hash
   - ‚úÖ **Actual built image**: `us-central1-docker.pkg.dev/jay-dhanwant-experiments/talking-head-registry/runpod-nvidia-smi:bd65ba12e2b2`
   - ‚ùå **Incorrect hash in Actions**: `8ff246d6b3354238e8c562f595c8c01bb2ca735`
   
   **Issue**: RunPod couldn't pull image due to typo in SHA hash from GitHub Actions summary
   
2. ‚úÖ **Manual endpoint creation** - COMPLETED (using correct image URL)
3. **Test nvidia-smi endpoint** - IN PROGRESS

### **New Discovery: Official CI/CD Support**
**Reference**: https://www.runpod.io/articles/guides/integrating-runpod-with-ci-cd-pipelines

RunPod **DOES support API-based CI/CD automation**! Our initial approach wasn't wrong, just poorly executed.

**Key Insights from Official Guide**:
- ‚úÖ **API integration is recommended** for CI/CD pipelines
- ‚úÖ **GitHub Actions workflows** are explicitly mentioned
- ‚úÖ **Automated deployment** is the intended use case
- ‚úÖ **REST API calls** can manage serverless endpoints

**Recommended CI/CD Flow**:
1. Build/push Docker image ‚úÖ
2. Send API request to RunPod to launch pod or update serverless endpoint ‚úÖ
3. Automated testing before deployment ‚úÖ
4. Handle authentication securely ‚úÖ

## Attempt 3: Full Automated CI/CD (IMPLEMENTED)
**Date**: Sept 2, 2024  
**Approach**: Complete automation using RunPod GraphQL API

### What We Built:
**File**: `.github/workflows/deploy-runpod.yml`

#### Workflow Steps:
1. **Build & Push Image** ‚Üí Google Artifact Registry
2. **Create Template** ‚Üí RunPod GraphQL `saveTemplate` mutation
3. **Create/Update Endpoint** ‚Üí RunPod GraphQL `saveEndpoint` mutation  
4. **Test Endpoint** ‚Üí Automated `runsync` API call
5. **Generate Summary** ‚Üí GitHub Actions summary with test commands

#### Key Features:
- ‚úÖ **Template Management**: Creates unique templates with timestamps
- ‚úÖ **Endpoint Management**: Creates new or updates existing endpoints
- ‚úÖ **Error Handling**: Validates API responses, fails on errors
- ‚úÖ **Automated Testing**: Tests endpoint after deployment
- ‚úÖ **Environment Support**: Production/staging/development environments
- ‚úÖ **GPU Configuration**: Configurable GPU types via workflow inputs

#### Secrets Required:
- `GCP_CREDENTIALS` - Google Cloud service account JSON
- `RUNPOD_API_KEY` - RunPod API key

### Expected Workflow Result:
```
‚úÖ Docker image built and pushed
‚úÖ RunPod template created: nvidia-smi-prod-1693689600  
‚úÖ RunPod endpoint created/updated: nvidia-smi-prod
‚úÖ Endpoint tested successfully
üìä GitHub Actions summary with test commands and dashboard links
```

### Expected Test:
```bash
curl -X POST "https://api.runpod.ai/v2/ENDPOINT_ID/runsync" \
  -H "Content-Type: application/json" \
  -d '{"input": {}}'
```

**Expected Response**:
```json
{
  "status": "COMPLETED",
  "output": {
    "gpu_info": "nvidia-smi output here..."
  }
}
```

---

## Key Learnings So Far:

### ‚úÖ Do's:
1. **Follow official documentation** - don't invent your own patterns
2. **Keep handlers simple** - one function, clear purpose
3. **Use RunPod Console** for endpoint creation and management
4. **GitHub integration** is preferred over API automation
5. **Test locally first** with simple docker run
6. **Minimal dependencies** - only what you need

### ‚ùå Don'ts:
1. **Mix frameworks** - don't combine FastAPI with RunPod handlers
2. **Complex GraphQL API calls** - use Console instead
3. **Multiple deployment methods** - pick one approach
4. **Ignore worker quotas** - monitor your limits
5. **Create duplicate endpoints** - clean up old ones
6. **Over-engineer** - start simple, add complexity later

### üîß RunPod Specific Gotchas:
1. **Queue-based vs Load Balancing** are fundamentally different
2. **Template names must be unique** across your account
3. **Worker quotas are shared** across all endpoints  
4. **GitHub releases** trigger automatic updates (not pushes)
5. **Image build timeout** is 160 minutes max
6. **Container size limit** is 80GB

---

## Attempt 4: GraphQL to REST API Migration (FAILED)
**Date**: Sept 3, 2025  
**Approach**: Migrating from GraphQL to REST API due to better documentation and modern API design

### Problem Encountered
When migrating from GraphQL to REST API, encountered schema validation error:

```json
[{"error":"At #/paths/templates/post for POST https://rest.runpod.io/v1/templates, The request body is defined as an object. However, it does not meet the schema requirements of the specification. Suggestion: Ensure that the object being submitted, matches the schema correctly","problems":["At /templates/properties/env/type: got array, want object"]}]
```

### What We Tried
1. **Migration Approach**: Changed API endpoints
   - Template creation: `https://api.runpod.io/graphql` ‚Üí `https://rest.runpod.io/v1/templates`
   - Endpoint creation: `https://api.runpod.io/graphql` ‚Üí `https://rest.runpod.io/v1/endpoints`
   - Endpoint listing: GraphQL query ‚Üí `GET https://rest.runpod.io/v1/endpoints`

2. **Request Format Used** (‚ùå INCORRECT):
   ```json
   {
     "name": "$TEMPLATE_NAME",
     "imageName": "$IMAGE_URL", 
     "dockerArgs": "",
     "containerDiskInGb": 10,
     "volumeInGb": 100,
     "env": [{"key": "CUDA_VISIBLE_DEVICES", "value": "0"}],  // ‚ùå WRONG: array format
     "containerRegistryAuthId": "cmf2s9lsv0001jl0267lkqfxq"
   }
   ```

### Root Cause
The REST API expects `env` to be an **object**, not an array of key-value pairs:
- ‚ùå GraphQL/Array format: `env: [{"key": "CUDA_VISIBLE_DEVICES", "value": "0"}]`
- ‚úÖ REST API/Object format: `env: {"CUDA_VISIBLE_DEVICES": "0"}`

### Solution Required
Update the workflow to use the correct REST API schema per official documentation:

```json
{
  "name": "template-name",
  "imageName": "image-url",
  "env": {
    "CUDA_VISIBLE_DEVICES": "0"  // Object format, not array
  },
  "containerDiskInGb": 10,
  "volumeInGb": 100,
  "containerRegistryAuthId": "auth-id"
}
```

### Official REST API Reference
Based on RunPod's documentation at `https://rest.runpod.io/v1/templates`:
```bash
curl --request POST \
  --url https://rest.runpod.io/v1/templates \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
    "name": "<string>",
    "imageName": "<string>", 
    "env": {
      "ENV_VAR": "value"  // Object format required
    },
    "containerDiskInGb": 50,
    "volumeInGb": 20
  }'
```

### Status  
**FULLY IMPLEMENTED**: Complete REST API migration with volume management.

### Issues Fixed
1. **Environment Variables Format**: Changed from array `[{"key":"value"}]` to object `{"key":"value"}`
2. **Invalid Fields**: Removed `dockerArgs` (not in schema)
3. **Volume Integration**: Added `volumeInGb`, `volumeMountPath`, `networkVolumeId` support

### Final Implementation
**Complete REST API Schema Used**:
```json
{
  "name": "$TEMPLATE_NAME",
  "imageName": "$IMAGE_URL",
  "containerDiskInGb": 20,
  "volumeInGb": 100,
  "volumeMountPath": "/workspace",
  "networkVolumeId": "$VOLUME_ID",
  "env": {"CUDA_VISIBLE_DEVICES": "0"},
  "containerRegistryAuthId": "auth-id"
}
```

**New Features Added**:
- ‚úÖ **Network Volume Management**: 100GB persistent storage
- ‚úÖ **Volume Reuse Logic**: Finds existing volumes by name
- ‚úÖ **Multi-tier Storage**: Container disk + local volume + network volume
- ‚úÖ **Datacenter Configuration**: US-KS-2 (configurable)
- ‚úÖ **Complete Error Handling**: Volume and template creation failures

## Attempt 5: Complete Volume-Enabled CI/CD (CURRENT)
**Date**: Sept 3, 2025  
**Approach**: Full REST API automation with comprehensive volume management for model storage

### Architecture Overview
**Multi-Tier Storage Strategy**:
1. **Container Disk**: 20GB temporary storage (wiped on restart)
2. **Local Volume**: 100GB at `/workspace` (persistent across restarts)  
3. **Network Volume**: 100GB shared storage for models (persistent across deployments)

### Workflow Implementation

#### Step 1: Network Volume Management
```bash
# Creates or reuses existing volume
POST /v1/networkvolumes
{
  "name": "model-storage-{env}",
  "size": 100,
  "dataCenterId": "US-KS-2"
}
```

#### Step 2: Template Creation with Volume Attachment
```bash
POST /v1/templates
{
  "name": "nvidia-smi-{env}-{timestamp}",
  "imageName": "{docker-image-url}",
  "containerDiskInGb": 20,
  "volumeInGb": 100, 
  "volumeMountPath": "/workspace",
  "networkVolumeId": "{volume-id}",
  "env": {"CUDA_VISIBLE_DEVICES": "0"},
  "containerRegistryAuthId": "{registry-auth-id}"
}
```

#### Step 3: Serverless Endpoint Creation
```bash
POST /v1/endpoints
{
  "name": "nvidia-smi-{env}",
  "templateId": "{template-id}",
  "gpuTypeIds": ["{gpu-type}"],
  "workersMin": 0,
  "workersMax": 2,
  "idleTimeout": 5,
  "scalerType": "QUEUE_DELAY",
  "scalerValue": 4
}
```

### Benefits of This Approach
- ‚úÖ **Persistent Model Storage**: 100GB network volume survives endpoint updates
- ‚úÖ **Cost Efficient**: Volume reuse prevents duplicate storage charges
- ‚úÖ **Flexible Storage**: Multiple mount points for different use cases  
- ‚úÖ **Scalable**: Network volumes can be shared across multiple endpoints
- ‚úÖ **Automated**: Zero manual intervention required

### Expected Deployment Flow
```
üì¶ Build Docker Image
  ‚Üì
üíæ Create/Find 100GB Network Volume  
  ‚Üì
üèóÔ∏è Create Template with Volume Attached
  ‚Üì
üöÄ Create/Update Serverless Endpoint
  ‚Üì
üß™ Test Endpoint Functionality
  ‚Üì
üìä Generate Deployment Summary
```

### Storage Access in Container
- **Network Volume**: Mounted at `/runpod-volume/` (100GB persistent)
- **Local Volume**: Mounted at `/workspace/` (100GB temporary persistent)
- **Container Disk**: Available at `/` (20GB temporary)

**Model Storage Recommendation**: 
- Store models in `/runpod-volume/` for persistence across deployments and endpoint updates
- Use `/workspace/` for temporary processing files
- Consider symlinking: `ln -s /runpod-volume /workspace/models` for convenience

**Important**: Network volumes attach to ENDPOINTS, not templates. Template creation was fixed to remove invalid `networkVolumeId` field.

### üéâ SUCCESS! Deployment Working
**Status**: ‚úÖ **FULLY OPERATIONAL** - Complete volume-enabled CI/CD working as expected!

**What Works**:
- ‚úÖ 100GB Network volume creation/reuse in configurable datacenter
- ‚úÖ Template creation with proper storage configuration (no invalid fields)
- ‚úÖ Endpoint creation with network volume attachment via `networkVolumeId`
- ‚úÖ Multi-tier storage: 20GB container + 100GB local + 100GB network volume
- ‚úÖ Proper mount paths: `/runpod-volume/` for network, `/workspace/` for local
- ‚úÖ Complete parameterization with configurable GPU types and datacenters
- ‚úÖ Comprehensive deployment summary with storage guidance

**Key Learning**: Network volumes must be attached to ENDPOINTS (via `networkVolumeId` in endpoint creation), NOT templates. Templates only support local storage configuration.

**Final Architecture**:
```
üèóÔ∏è Template: Container disk (20GB) + Local volume (100GB at /workspace)
üì° Endpoint: Template + Network volume (100GB at /runpod-volume) 
üöÄ Result: Multi-tier persistent storage for serverless workloads
```

---

## Summary of All Attempts

### ‚ùå Attempt 1: Complex FastAPI + Load Balancing (FAILED)
- Mixed patterns, GraphQL errors, worker quota issues

### ‚ùå Attempt 2: Manual Console Deployment (PARTIAL)
- Following official docs, manual endpoint creation

### ‚úÖ Attempt 3: GraphQL Automation (WORKED)
- Full automated CI/CD with GraphQL API

### ‚ùå Attempt 4: REST API Migration (FAILED)
- Schema validation errors, invalid field formats

### üéâ Attempt 5: Volume-Enabled REST CI/CD (SUCCESS!)
- Complete automation with proper volume management
- Correct endpoint-level network volume attachment  
- Multi-tier storage architecture
- Full parameterization and error handling

**Final Result**: Production-ready RunPod serverless deployment with 100GB persistent storage! üöÄ

---

## Attempt 6: Timeout Configuration for Long-Running Tasks
**Date**: Sept 3, 2025  
**Issue**: Model downloading from GCS will take significant time, but RunPod serverless has timeout limits

### üö® **Timeout Problem Identified:**
- **Default Execution Timeout**: 600 seconds (10 minutes)
- **Model Download Reality**: 30+ minutes for large model collections
- **Risk**: Downloads failing due to timeout, wasting compute and incomplete transfers

### ‚úÖ **Solution Implemented:**

#### **1. Added Configurable Execution Timeout**
```yaml
# New workflow input
execution_timeout:
  description: "Execution timeout (seconds)"
  default: "3600" 
  options:
    - "600"    # 10 minutes
    - "1800"   # 30 minutes  
    - "3600"   # 1 hour (default)
    - "7200"   # 2 hours
    - "14400"  # 4 hours
    - "86400"  # 24 hours
```

#### **2. Updated Endpoint Creation**
```json
{
  "executionTimeout": 3600,  // ‚úÖ Added timeout parameter
  "idleTimeout": 5,
  "scalerType": "QUEUE_DELAY"
}
```

#### **3. Enhanced Deployment Reporting**
- Execution timeout shown in deployment logs
- Timeout setting included in GitHub Actions summary
- Clear visibility into current timeout limits

### üéØ **Timeout Strategy:**
- **Default**: 1 hour (3600s) - handles most model downloads
- **Conservative**: 30 minutes for smaller models  
- **Large Models**: 2-4 hours for extensive collections
- **Maximum**: 24 hours for massive datasets

### üìä **Expected Performance:**
```
Small Models (<1GB):     10-30 minutes
Medium Models (1-5GB):   30-60 minutes  
Large Collections (>5GB): 1-4 hours
Massive Datasets (>20GB): 4+ hours
```

This ensures model downloads complete successfully without timeout failures! ‚è±Ô∏è

---

## Final Updates: Authentication & File Management
**Date**: Sept 3, 2025  
**Enhancement**: Added Bearer token authentication and file listing capabilities

### üîê **Authentication Requirements:**
RunPod serverless endpoints require Bearer token authentication for API calls.

#### **API Key Setup:**
```bash
# Set your RunPod API key as environment variable
export RUNPOD_API_KEY=your_runpod_api_key_here
```

#### **Authenticated API Calls:**
```bash
curl -X POST "https://api.runpod.ai/v2/ENDPOINT_ID/runsync" \
  -H "Authorization: Bearer $RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"input": {"test": "nvidia-smi"}}'
```

### üìÇ **New File Management Endpoint:**
Added `list_files` action for directory exploration and model verification.

#### **File Listing Usage:**
```bash
# List workspace files
curl -X POST "https://api.runpod.ai/v2/ENDPOINT_ID/runsync" \
  -H "Authorization: Bearer $RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"input": {"action": "list_files", "path": "/workspace"}}'

# List downloaded models  
curl -X POST "https://api.runpod.ai/v2/ENDPOINT_ID/runsync" \
  -H "Authorization: Bearer $RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"input": {"action": "list_files", "path": "/runpod-volume"}}'
```

#### **File Listing Response:**
```json
{
  "status": "success",
  "path": "/runpod-volume/models",
  "directories": [
    {"name": "musetalk", "path": "/runpod-volume/models/musetalk", "type": "directory"}
  ],
  "files": [
    {"name": "model.pth", "path": "/runpod-volume/models/model.pth", "size": 2048576, "type": "file"}
  ],
  "total_files": 47,
  "total_directories": 8
}
```

### üß™ **Complete Testing Workflow:**

#### **1. Test GPU Access:**
```bash
curl -X POST "https://api.runpod.ai/v2/ENDPOINT_ID/runsync" \
  -H "Authorization: Bearer $RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"input": {"test": "nvidia-smi"}}'
```

#### **2. Download Models (with timeout):**
```bash
curl -X POST "https://api.runpod.ai/v2/ENDPOINT_ID/runsync" \
  -H "Authorization: Bearer $RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"input": {"action": "download_models"}}'
```

#### **3. Verify Downloaded Models:**
```bash
curl -X POST "https://api.runpod.ai/v2/ENDPOINT_ID/runsync" \
  -H "Authorization: Bearer $RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"input": {"action": "list_files", "path": "/runpod-volume"}}'
```

### üìä **Expected Model Structure:**
```
/runpod-volume/models/
‚îú‚îÄ‚îÄ duwgan/           (Deep learning models)
‚îú‚îÄ‚îÄ face-parse-bisent/ (Face parsing models)  
‚îú‚îÄ‚îÄ musetalk/         (Audio-visual models)
‚îú‚îÄ‚îÄ musetalkV15/      (Updated models)
‚îú‚îÄ‚îÄ sd-vae/           (Stable diffusion VAE)
‚îú‚îÄ‚îÄ syncnet/          (Audio-video sync)
‚îî‚îÄ‚îÄ whisper/          (Speech recognition)
```

### ‚úÖ **Production Ready Features:**
- üîê **Authentication**: Bearer token security
- ‚è±Ô∏è **Configurable Timeouts**: 1-24 hour execution limits  
- üíæ **Persistent Storage**: 100GB network volume
- üìÇ **File Management**: Directory listing and exploration
- üß™ **Complete Testing**: GPU, download, and verification endpoints
- üöÄ **Auto-scaling**: 0-2 workers with queue-based scaling

**Status**: Fully operational serverless deployment with comprehensive model management! üéâ